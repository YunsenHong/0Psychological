\documentclass[11pt]{report}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand{\Ep}{\mathbb{E}}
\newcommand{\Real}{\mathcal{R}}

\begin{document}
\title{Development and Derivation of the Psychological Model}
\maketitle

This document provides development logs of the psychological model, including intuitions, derivations, and result summaries.

To begin with, we must outline several key concepts in building the blocks of our models. 
\begin{itemize}
\item $K$ aspects. We assume there are $K$ aspects based on which a user measure the items. The number $K$ is predefined. Consequently. we define the variables, $U,V$ where  $v_{i,k}$ denotes item $i$'s feature on aspect $k$ and $u_{p,k}$  denotes user $p$'s preference on aspect $k$. The range of $U,V$ is specified by the model, thus a probabilistic generative model and a numerical model might provide apply different range constraints.
\item Prominent aspect(s). We assume that the $K$ aspects are not treated equally by the users. One or some (depending on the model) of the aspects are most important. 
\item Distinguish prominent and non-prominent aspects. We adopt different scoring or ranking methods for prominent and non-prominent aspects. This part essentially makes our model non-compensate.   
\end{itemize}

\part{Ranking Based Models}
This part devotes to models that observe rankings. In each session, we assume that the user is given a set of items, and the user generates a set of pairwise rankings. For example, if a user has access to items $i,j$, and he buys $i$, clicks on $j$. Then we can make a reasonable conclusion that to the user $i\succ j$. The ranking based models mimic the generation of pairwise rankings. The details of observing rankings, i.e. which actions to be taken into accounts and how to grade these actions, are out of the scope of this document (but should be considered in the experiments).

\section{The BTL Model}
%瑛珈，非生成模型，非贝叶斯

%严格？宽松？

%we use the BTL model with ties $p(i\succ j|g_k=1)=\frac{\theta v_i}{\theta v_i+v_j}$. Therefore, on the prominent aspects, the winning product is likely to be better than the losing product, but on the non-prominent aspects, the winning product can be not as good as the losing product as long as $\theta v_i \succ  v_j$. For simplicity, the value of $\theta>1$ is pre-set. Below, we will use $o$ to denote the observations. 

\section{The Sigmoidal Model}
As the section title indicates, we propose a set of models that relate pair-wise rankings $i\succ j$ to logit functions. The optimization objective is in a general form of :

\begin{eqnarray}
\min_{u,v} & \sum_{i\succ j} \frac{1}{2} (1-\sum_k u_k \sigma(v_{i,k}-v_{j,k}))^2 - \frac{\lambda}{2} \sum_u \|u\|^2 \\\nonumber
 s.t. & \forall u \forall k, u_k>0\\\nonumber
  & \forall u, \sum_k u_k=1
\end{eqnarray}

There are a few issues needed to be emphasized here. (1) Essentially this is a mixture model so only evaluation on prominent aspects is provided. We could make explicit treatments for prominent and non-prominent aspects in the model by changing the constraints to $\forall u,\forall k, 0<u_k<1$. (2) With the negative regularization term (together with the constraints) we penalize sparse $u$s, thus we allow more prominent aspects to be chosen. (3) We use the sum-of-square error function, because our prediction for a pair $<i,,j>$$f(i j)=\sum_k u_k \sigma(v_{i,k}-v_{j,k})$ falls in the range of $(0,1)$ and $f(i,j)=1-(j,i)$. So we label $i\succ j$ to be 1. For symmetry, we don't have to label $j\succ i$. (4) We do not normalize $v$ as $\sigma(v_{i,k}, v_{j,k})$ is already normalized.

The optimization algorithm is listed below. In each iteration, we first optimize over $u$ when $v$s are fixed. We rewrite the objective as $\|Ax-b\|^2-\lambda \|x\|^2$ with constraints $C_i^Tx=d_i, \forall i=1,\cdots,U$, where $x\in \Real^{KU}$, $U$ is the number of users, $x_i=u_{i,k}$, $A\in \Real^{D}$, $D$ is the number of observations and $|D|\simeq SL$, where $S$ is the number of sessions, and $L$ is the average number of pairs in a session, $C_i\in \Real^{KU}$ where for $1\leq i \leq U, iK+1\leq j \leq i(K+1), C_{i,j}=1 \forall j', C_{i,j'}=0$, $b$ is a vector of all ones, $d_i=1$ is a scalar. Therefore we have

\begin{equation}
\begin{bmatrix}
x^* \\
z^*
\end{bmatrix} = \begin{bmatrix}
2A^TA-\lambda & C^T \\
C & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
2A^Tb \\
d
\end{bmatrix}
\end{equation}
Note that we have $S\geq U$ to filter cold start users, and generally $L\geq K-1$, $K>1$, therefore the matrix is invertible. 

Next we optimize $v$ while keeping $u$ fixed. We adopt stochastic gradient.  As shown below, the gradient is the addiction of two terms:

\begin{eqnarray*}
\frac{\delta L}{\delta v_{i,k}}&=&\sum_{i\succ j} (1-\sum_{k'} u_{k'}\sigma(v_{i,k'},v_{j,k'}))\sigma(v_{i,k},v_{j,k})(1-\sigma(v_{i,k},v_{j,k}))\\
&&+ \sum_{w\succ i} (1-\sum_{k'} u'_{k'}\sigma(v_{w,k'},v_{i,k'}))\sigma(v_{w,k},v_{i,k})(\sigma(v_{w,k},v_{i,k})-1)
\end{eqnarray*}

We can sample an item $j$ from $i\succ j$ and another item $w$ from $w\succ i$,  compute $S_1= (1-\sum_{k'} u_{k'}\sigma(v_{i,k'},v_{j,k'}))\sigma(v_{i,k},v_{j,k})(1-\sigma(v_{i,k},v_{j,k}))$ and $(1-\sum_{k'} u'_{k'}\sigma(v_{w,k'},v_{i,k'}))\sigma(v_{w,k},v_{i,k})(\sigma(v_{w,k},v_{i,k})-1)$

\begin{equation}
v_{i,k}=v_k+\eta |i\succ j| S_1 +\eta |w\succ i| S_2
\end{equation}


      
\part{Rating Based Models}
In this section, we develop a series of ranking based models. These models have the following properties. (1) The models are based on rating observations, i.e. we assign a rating to every action. The actions include the old-fashioned preference ratings (in such case, each rating is a standalone session), actions in a session, positive and negative samples, etc. (2) The models are Bayesian generative models that have good explainability. (3) We control the assumptions by adjusting the hyper-parameters in these models.
   
\section{The Beta model}
%motivations
 We are motivated to design this model by the following assumptions: (1) we allow  multiple ($0~K$) aspects in a session to be prominent. (2) We use positive and negative observations, e.g. a click is negative and a buy is positive. (3) For prominent aspects, we directly model the likelihood by the Beta function. (4) For non-prominent aspects, we assume that they do not contribute to the likelihood of being positive or negative, ie. $p(o=1|g_k=0)=0.5$.  (5) We assume that the final label is assigned by a very strict measurement, i.e. the item must be ``good'' on all aspects.
 
 %experiments
As such, in experiments, we should monitor model performances on the following issues. (1) A proper baseline would be to also treat the recommendation system as binary classification problems. For simplicity, I would recommend starting with those traditional MF or classifiers without any number, or time related features. (2) The binary classification framework is easily extend to one class classification scheme for implicit feedback. However, I would recommend to first testify the performance for two classes. When we involve implicit feedback, a fair comparison is important. It would be best if we could verify the capacity of our model on two-class problems, before we use confidence levels or sampling techniques for the negative implicit feedback. (3) Shall we induce sparse constraints on the number of prominent aspects? This could be done by choosing proper values for $a,b$.  (4) Tuning the parameter $K$ is the last step in experimental studies.
 
 %future extensions
For future extensions, we should keep in mind that this model might suffer from the following weaknesses. (1) The likelihood is obtained by a multiplication of all aspects. Although this is proved by preceding pilot studies, that a complete BTL models multiplying all aspects is better than a simplified BTL model on just one aspect, I am not very confident about the conclusion. We can easily modify the model to account for an opposite assumption, that the ``good'' performance on one aspect can override other aspects.  (2) The non-prominent aspects are discarded. This could be fixed by introducing a step function, i.e. $p(o_v=1|g_k=0)=f(v-\theta)$.  (3)  Shall we consider ratings (normalized to $(0,1)$), instead of positive and negative samples? A fair comparative study with MF is needed to answer this question.  If the answer is, we can modify the generation of observations. All the above directions are possible and can be implemented without much difficulty.
 
\begin{figure}
  \centering
  \tikz{ %
%users and items
    \node[const] (a) {$a$} ; %
    \node[const, right=2 of a] (b) {$b$} ; 
    \node[const, right =2 of b] (alpha) {$\alpha$};
       \node[const, right =2 of alpha] (beta) {$\beta$};
     \node[latent, below =of a] (u1) {$u_1$};
     \node[const, right = of u1] (dots) {$\cdots $};
        \node[latent, right = 2 of u1 ] (uK) {$u_K$};
        
  	\node[latent, right = of uK] (v1) {$v_1$};
	     \node[const, right = of v1] (vdots) {$\cdots $};
        \node[latent, right = 3 of v1 ] (vK) {$v_K$};
        
       	\edge{a}{u1};
	\edge{b}{u1};
	\edge{a}{uK};
	\edge{b}{uK};
	
	 \plate[inner sep=0.1cm, xshift=-0cm, yshift=0.12 cm] {U} {(u1) (dots) (uK) } {M}; 
	 \edge{alpha}{v1};
	\edge{alpha}{vK};
	\edge{beta}{v1};
	\edge{beta}{vK};
       \plate[inner sep=0.1cm, xshift=-0cm, yshift=0.12 cm] {V} {(v1) (vdots) (vK) } {N}; 
       %per session
       
     \node[latent, below = 2 of u1] (g1) {$g_1$};    
     	     \node[const, right = of g1] (gdots) {$\cdots $};
        \node[latent, right = 2 of g1 ] (gK) {$g_K$};
           
           	  \node[latent,  below = of g1] (h1) {$h_1$};    
                	     \node[const, right = of h1] (hdots) {$\cdots $};
        \node[latent, right = 2 of h1 ] (hK) {$h_K$};
           \node[obs, below =  of hdots] (o) {$o$};  
           
	 
	
	  \edge{u1}{g1};
	    \edge{uK}{gK};
	    
		  \edge{v1}{h1};
	  	  \edge{g1}{h1};
		  
		  	  \edge{vK}{hK};
	  	  \edge{gK}{hK};
		  
	  \edge{h1}{o};
	   \edge{hdots}{o};
	    \edge{hK}{o};
	    
	

	    \plate[inner sep=0.1cm, xshift=0cm, yshift=0.12 cm] {S} {(o) (h1) (hdots) (hK) } {L}; 
	    \plate[inner sep=0.4 cm, xshift=-0.12cm, yshift=0.12 cm] {D} {(g1) (gdots) (gK) (S) } {S}; 
 }
 \caption{Plate notation of the proposed sigmoidal model}\label{fig:sigmoidal}
\end{figure}

First we generate the user preferences and item features. 
\begin{itemize}
\item For aspect $k=1: K$, for user $u=1: M$, sample user preference on aspect $k: u_k\sim Beta(a,b)$. Unlike most of previous research, the user preference vector $u$ in our work is not the normalized weight over all aspects.
\item For items $v=1: N$, sample item features for the item universe $v_k \sim Beta (\alpha,\beta)$. Again, unlike most of previous research, the user preference vector $u$ in our work is not the normalized weight over all aspects.

\end{itemize}

Next, given $S$ sessions of user $u$
\begin{itemize}
\item For each session $s$, for aspect $k=1:K$, generate aspect indicator $g_{s,k} \sim Bern(u_k)$. If $g_{s,k}=1$, then $k$ is a prominent aspect for user $u$ in the session $s$.
\item For each item $v$ within the session $s$ of length $L$
\begin{itemize}
\item For each aspect $k$, generate the indicator $p(h_{s,v,k}|g_{s,k},v_k)=[v_k^{h_{s,v,k}} (1-v_k)^{1-h_{s,v,k}}]^{g_{s,k}} [\frac{1}{2}^{h_{s,v,k}} \frac{1}{2}^{1-h_{s,v,k}}]^{1-g_{s,k}}$
%no sigmoiidal $p(h_{s,v,k}=1|v_k,g_k)=\frac{1}{1+\exp{v_k}}^{g_{s,k}}\frac{1}{2}^{1-g_{s,k}}$
\item For each observed rating $o_v$, generate label observation $o_{s,v}=1-\prod_k (1-h_{s,v,k})$ where $o_v=1$ if and only if $\forall k, h_{s,v,k}=1$
\end{itemize}
\end{itemize}

The joint probability is given by
\begin{equation*}
    p(U,V,G,H,D |a,b,\Lambda) =    \prod_k \prod_u p(u_{k}|a,b)  \prod_{s} p(g_{s,k} | u_k)  \prod_{v} \{ p(v_k|0,\alpha,\beta) p(o_{s,v}|H)  p(h_{s,v,k}|g_{s,k},v_k)\}
    \end{equation*}

%Sigmoidal 
%where the upperbound for $p(h_{s,vk}|g_{s,k},v_k)$is
%\begin{eqnarray*}
%p(h_{s,v,k}|g_{s,k},v_k) &=& \frac{1}{2}^{1-g_{s,k} }[\sigma(v_k)^{h_{s,v,k}} (1-\sigma(v_k))^{1-h_{s,v,k}} ]^{g_{s,k}}\\
%&\leq & \frac{1}{2}^{1-g_{s,k} } \exp(v_k h_{s,v,k}) \sigma(\xi) \exp[ -\frac{v_k+\xi}{2} - \lambda(\xi) (v_k^2- \xi^2) ]
%\end{eqnarray*}

We should notice that $o_{v,k}=1$ indicates $\forall k, h_{v,k}=1$. Thus the likelihood must be decomposed to a term with observations and a term with hidden variables. We have:

\begin{eqnarray}\label{equ:completelikelihoodsigmodal}
\ln p(U,V,G,H,D|a,b,\Lambda,\xi)  & = &\sum_s\sum_{o_{s,v}=1} \sum_k p(h_{s,v,k}=1|g_{s,k},v_k) \\\nonumber
& &+\sum_s \sum_{o_{s,v}=0} p(o_{s,v}=0|H) \sum_k p(h_{s,v,k}|g_{s,k},v_k)\\\nonumber
& &+\sum_u\sum_k p(u_{k}|a,b) + \sum_v \sum_k p(v_k|\alpha,\beta) + \sum_u \sum_s \sum_k p(g_{s,k}|u_k) 
\end{eqnarray}

By variational inference, we have factorize the posterior probability $p(U,V,G,H|D,a,b,\alpha,\beta )=q(U)q(V)q(G)q(H)$. We can see from the following derivations that $q(u_k)\sim Bern(a',b')$
\begin{eqnarray*}
\ln q(u_k) & = & \mathbb{E}_{G,V,H} [\ln p(u_k|a,b)+\sum_s \ln p(g_{s,k}|u_k)]+const\\
&=&(a-1)\ln u_k +(b-1) \ln (1-u_k) + \sum_{s} \mathbb{E}[g_{s,k}] \ln u_k + \sum_s (1-\mathbb{E}[g_{s,k}] \ln (1-u_k) + const \\
\mathbb{E}[u_k] &=& \frac{a+\sum_{s} \mathbb{E}[g_{s,k}] }{a+b+ |S|  }
\end{eqnarray*}

%Sigmoidal: Because $p(h_{s,v,k}=1|g_{s,k},v_k) = \frac{1}{2}^{1-g_{s,k}} \sigma(v_k) g_{s,k} $, $\ln p(h_{s,v,k}=1|g_{s,k},v_k) \geq g_{s,k} [\frac{v_k-\xi}{2} -\lambda(\xi) (v_k^2-\xi^2)]$
Because $p(h_{s,v,k}|g_{s,k},v_k)=[v_k^{h_{s,v,k}} (1-v_k)^{1-h_{s,v,k}}]^{g_{s,k}} [\frac{1}{2}^{h_{s,v,k}} \frac{1}{2}^{1-h_{s,v,k}}]^{1-g_{s,k}}$, we have $\ln p(h_{s,v,k}|g_{s,k},v_k)= g_{s,k} [h_{s,v,k}\ln v_k + (1-h_{s,v,k}\ln (1-v_k)] + (1-g_{s,k}\ln\frac{1}{2}$
%Sigmoidal
%\begin{eqnarray*}
%\ln q(v) & = & \mathbb{E}_{G,U,H} [\ln p(H|v,G)+\ln(v|\alpha,\beta) ] + const\\
%&=& \sum_u \sum_s \sum_k \{ \mathbb{E}[g_{s,k}] [v_kh_{s,k} -\frac{v_k+\xi}{2} -\lambda(\xi) (v_k^2-\xi^2) ] -\frac{1}{2} v^T\Lambda v\} + const\\
%&=& \sum_s\sum_{o_{s,v}=1} \sum_k \mathbb{E} [g_{s,k}] [\frac{v_k-\xi}{2} -\lambda(\xi) (v_k^2-\xi^2)] + 
%\end{eqnarray*}

\begin{eqnarray*}
\ln q(v) & = & \mathbb{E}_{G,U,H} [\ln p(H|v,G)+\ln(v|\alpha,\beta) ] + const\\
&=& \sum_s\sum_{o_{s,v}=1} \sum_k p(h_{s,v,k}=1|g_{s,k},v_k) + \sum_s\sum_{o_{s,v}=0} \sum_k p(h_{s,v,k}|g_{s,k},v_k) +  \sum_v \sum_k p(v_k|\alpha,\beta) \\
\ln q(v_k) & = & \{\sum_s\sum_{o_{s,v}=1}  \mathbb{E}[g_{s,k}] + \sum_s\sum_{o_{s,v}=0}  \Ep[g_{s,k}]\Ep[h_{s,v,k}]\} \ln v_k \\
& & + \sum_s\sum_{o_{s,v}=0} \Ep[g_{s,k}]\Ep[1-h_{s,v,k}] \ln (1-v_k) +  (\alpha-1)\ln v_k + (\beta-1) \ln (1-v_k) + const \\
\Ep[v_k]& = & \frac{\sum_s\sum_{o_{s,v}=1}  \mathbb{E}[g_{s,k}] + \sum_s\sum_{o_{s,v}=0}  \Ep[g_{s,k}]\Ep[h_{s,v,k}]+\alpha}{\sum_s\sum_{o_{s,v}=1}  \mathbb{E}[g_{s,k}] + \sum_s\sum_{o_{s,v}=0}  \Ep[g_{s,k}]\Ep[h_{s,v,k}]+\alpha+\beta + \sum_s\sum_{o_{s,v}=0} \Ep[g_{s,k}]\Ep[1-h_{s,v,k}]}
\end{eqnarray*}



\begin{eqnarray*}
\ln q(G) & = & \mathbb{E}_{U,V,H} (\ln p(G|U)+\ln (H|G,U))+ const \\
\ln q(g_{s,k}) & = & \ln p(g_{s,k}|u_k) + \sum_{o_{s,v}=1}  p(h_{s,v,k}=1|g_{s,k},v_k) + \sum_{o_{s,v}=0}  p(h_{s,v,k}|g_{s,k},v_k) \\
&=& g_{s,k} \{\Ep[\ln u_k] + \Ep[\ln v_k] + \sum_{o_{s,v}=1}  \Ep[\ln v_k] + \sum_{o_{s,v}=0} [ \Ep[h_{s,v,k}] \Ep[\ln v_k] + \Ep[1-h_{s,v,k} ] \Ep[1-\ln v_k] ] \}\\
& & + (1-g_{s,k}) \{\Ep[\ln (1-u_k)] + \sum_{o_{s,v} } \ln \frac{1}{2}  \}  \\
\mathbb{E}[g_{s,k}]&=&\frac{\Ep[\ln u_k] + \Ep[\ln v_k] + \sum_{o_{s,v}=1}  \Ep[\ln v_k] + \sum_{o_{s,v}=0} [ \Ep[h_{s,v,k}] \Ep[\ln v_k] + \Ep[1-h_{s,v,k} ] \Ep[1-\ln v_k] ] }{\Ep[\ln u_k] + \Ep[\ln v_k] + \sum_{o_{s,v}=1}  \Ep[\ln v_k] + \sum_{o_{s,v}=0} [ \Ep[h_{s,v,k}] \Ep[\ln v_k] + \Ep[1-h_{s,v,k} ] \Ep[1-\ln v_k] ]  +(1-g_{s,k}) \{\Ep[\ln (1-u_k)] + \sum_{o_{s,v} } \ln \frac{1}{2}  \}  }
\end{eqnarray*}

where $\Ep[\ln(u_k) = \phi(a')\phi(a'+b'), \Ep[\ln(v_k) = \phi(\alpha')\phi(\beta')$.  

\begin{eqnarray*}
\ln q(H) & = & \mathbb{E}_{G,U,V} (\ln p(H|V,G) + \ln p(O|H)) \\
\ln q(h_{s,v,k}=1) &=& \sum_{o_{s,v}=1} \sum_k p(h_{s,v,k}=1|g_{s,k},v_k) +\sum_{o_{s,v}=0} \sum_{h_{s,v,k'}} p(o_{s,v}=0|H)  p(h_{s,v,k'}|g_{s,k'},v_k) + const\\
\Ep [h_{s,v,k} ] &= &   q(h_{s,v,k}=1) + q(h_{s,v,k}=0) 
\end{eqnarray*}



\end{document}
\documentclass{article}
\usepackage[fleqn]{amsmath}


\title{A Detailed Derivation of the Psychological Model}
\begin{document}
\section{Explicit Feedback}
For explicit feedback, we assume that a pair of purchase and click $<w,l>$ is the result that (1) $w$ is superior than $l$ on the most pertinent aspect $g_k=1$ and (2) $w$ at least ties with $l$ on other aspects. 

We have the following equation: 
\begin{equation}
p(<w,l>|\Theta,g) = \prod_{k=1}^K {\frac{w_k}{w_k+\theta l_k}}^{g_k} {\frac{\theta w_k}{l_k + \theta w_k}}^{1-g_k},
\end{equation} 
 
where $g$ the hidden variable is sampled from $u$, $\Theta=\{\theta,w_k,l_k, u\}$ is the parameter space.

The likelihood over all sessions is defined by

\begin{equation}\label{equ:likelihood}
p(D|\Theta)= \prod _u \prod_{d\in D(u)} \Sigma_k (\prod_{w\in W(d)} \prod_{l \in L(d)} p(<w,l>|\Theta,g) p(g|u))
\end{equation} 

Due to the summation part, the log-likelihood can not be analytically optimized. To maximize the log-likelihood, we follow the EM framework.

\textbf{E-step} 
\begin{eqnarray}
p(g|d,\Theta^{t}) & \propto   p(g|u,\Theta^{t}) p(d|g,\Theta^{t}) \\ \nonumber
 p(g_k=1|d,\Theta^{t}) & \propto  u_k^{t} \frac{w_{k}^t} {w_{k}^t+\theta^t l_{k}^t} \prod_{k'\neq k}  [\frac{\theta^t w_{k'}^t} {l_{k'}^t + \theta^t w_{k'}^t}]
\end{eqnarray} 

\textbf{M-step} 

For any positive variables $x,y$, we have
\begin{equation*}
\ln \frac{y}{x} \geq 1- \frac{x}{y}
\end{equation*}

Therefore we can derive a lower bound for the log-likelihood over the complete data, given the parameters learnt from previous round. 

\begin{eqnarray}
l(\Theta,g) & = \sum_u \sum_{d\in D(u)} \log \{ u_k  \frac{w_{k}} {w_{k}+\theta l_{k}} \prod_{k'\neq k}  [\frac{\theta^t w_{k'}} {l_{k'} + \theta^t w_{k'}}] \} \\\nonumber
& \geq  Q^t(\Theta,g)\\\nonumber
& \sum_u \sum_{d\in D(u)} \{\log  u_k  + [ 1- \frac{w_k+\theta l_k}{ w_k^t + \theta^t l_k^t}  + \log \frac{w_{k}} {w_{k}^t+\theta ^t l_{k}^t} ] \sum_{k'\neq k}  [ 1- \frac{ l_{k'}+\theta w_{k'}}{ l_{k'}^t+\theta^t w_{k'}^t}  + \log \frac{\theta w_{k'}} {l_{k'}^t+\theta ^t w_{k'}^t} ] \}
\end{eqnarray}
 where $l_k^t, w_k^t,\theta^t$ are the parameters learnt from $t-th$ round.
  
The objective in M-step is $ Q^t= \sum_u \sum_{d\in D(u)} p(g_k=1|d,\Theta^{t})  Q^t(\Theta,g)$.   To maximize $Q^t$ we adopt coordinate descent.
  
 Fix $v$ for all $w$ and $l$ and $\theta$, we update $u$ by. (See the corresponding derivation in our paper)
 
 Fix $u,\theta$, we update $w$ and $l$ by.
 
 Fix $u,v$, we update $\theta$ by.
 
 \textbf{Stochastic S-step} 
 In stochastic EM, we simply add an S-step after the E-step. We draw the value of $g$ for each session $d$ by
 \begin{equation}
 k \sim u_k^{t} \frac{w_{k}^t} {w_{k}^t+\theta^t l_{k}^t} \prod_{k'\neq k}  [\frac{\theta^t w_{k'}^t} {l_{k'}^t + \theta^t w_{k'}^t}].
 \end{equation}
 Next replace the M-step by
 \begin{equation}
 \sum_u \sum_{d\in D(u)} \{[ 1- \frac{w_k+\theta l_k}{ w_k^t + \theta^t l_k^t}  + \log \frac{w_{k}} {w_{k}^t+\theta ^t l_{k}^t} ] \sum_{k'\neq k}  [ 1- \frac{ l_{k'}+\theta w_{k'}}{ l_{k'}^t+\theta^t w_{k'}^t}  + \log \frac{\theta w_{k'}} {l_{k'}^t+\theta ^t w_{k'}^t} ] \}
 \end{equation}
\section{Implicit Feedback}
\end{document}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} 
\usepackage{amsmath}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{algorithm2e}

\setcopyright{none}

\begin{document}

\title{A Psychological Model for Consumption Prediction}


\author{Paper 277}


\begin{abstract}
The study of consumer psychology reveals two categories of procedures used by consumers to make consumption related choices: compensatory rules and non-compensatory rules. Existing recommendation models such as matrix factorization assume the consumers follow the compensatory rules, which is to make decisions based on a weighted or summated score over different attributes. In this paper, we present a novel model which adopts non-compensatory decision rules. The selected item is superior on the most  important attribute and beyond the minimally acceptable level on other attributes. Furthermore, we incorporate ordinal utility in the psychological model for consumption prediction in click sessions. We experimentally demonstrate that this model outperforms state-of-the-art methods.
\end{abstract}

\keywords{consumer psychology, non-compensatory decision rules, ordinal utility, early prediction}

\maketitle
\section{Introduction}\label{sec:introduction}
%consumer psychology: overview of decision rules

Ever since the dawn of psychological science, many attempts have been made to explain human choice behavior. It is well regarded that there are two categories of procedures to make consumption related choices: compensatory rules and non-compensatory rules~\cite{Engel1986Consumer}. Consumers who adopt compensatory rules evaluate every product over multiple relevant aspects and compute a weighted or summated score for each product. Then they will select the products with highest scores. The shortcomings of a product are balanced out by its attractive features. On the contrary, non-compensatory rules do not allow a good performance on one aspect of a product to compensate for poor performances on other aspects. Non-compensatory rules include lexicographic, conjunction and disjunction rules. Under lexicographic rules, products are compared on the most important aspect. Under conjunction  and disjunction rules, the consumer imposes requirements for minimally acceptable values on each aspect separately.  The conjunction and disjunction rules are often used in conjunction with other decision rules.
%previous works 

One essential goal of recommender systems (RS) is to understand consumption patterns. In the fruitful literature of RS, almost all models are implementations of the compensatory decision rules, i.e. they assume that the probability of a consumption is calculated by a weighted combination of item performances over several hidden aspects~\cite{Hu2008Collaborative,Gopalan2015Scalable}.  To the best of our knowledge, there is no model that is based on non-compensatory decision making rules. 

However, many researches in the field of consumer psychology revealed that users use both compensatory and non-compensatory rules~\cite{Engel1986Consumer}. The selection of decision rules is affected by many factors.  For example,  a survey ~\cite{Park1976Effect} pointed out that when the consumer is familiar with the product and the product is low in  complexity, he is most likely to establish a set of evaluative criteria and adopt conjunction rules.    

%first contribution

Our first contribution in this work is a novel probabilistic model that represents the cognitive process of consumers following non-compensatory rules. The problem being tackled here is to predict what item the consumer will buy in an activity session (i.e. a set of clicks on a e-commerce website). We assume that an item is purchased because (1) it is superior on the aspect which is currently of the most pertinence to the consumer, and (2) it surpasses  a minimum level of acceptable performance on other aspects. 

%ordinal utility
When modeling consumers' decision making process, another important issue is that consumers' preferences are described by ordinal utility rather than cardinal utility.  Ordinal utility is the idea that user's satisfaction (utility) cannot be measured in absolute quantity, while cardinal utility theory presumes utility is measurable and additive.  Suppose there are two items to evaluate, it is possible to ask a consumer which item is better, but it is meaningless to ask him how much better it is. In modern economics, ordinal utility theory is applied to study consumer behavior.

%ordinal utility in RS
RS models are usually expressed in terms of cardinal utility, largely because RS models are dealing with choices under uncertainty.  In the few recent works~\cite{Frolov2016Fifty} which employ ordinal utility, only the final decisions (i.e. differences among explicit ratings) are considered to be in the domain of ordinal variables. 

%motivation of ordinal utility
According to study of consumer behavior~\cite{Engel1986Consumer}, after a desire of purchase is triggered, the consumer will proceed with a series of alternative evaluations using limited memory and attention.  Our hypothesis in this paper is that ordinal utility is applied by the consumer throughout the decision process. We give an example below to show that, when coupling with non-compensatory rules, if the model does not reflect the complete decision making process, the prediction might be misleading.


\emph{Suppose there are 5 options $<A,B,C>$, exposed to the customer sequentially. The consumer evaluates each option on two attributes. }




%


%second contribution

Our second contribution is to incorporate ordinal utility. We assume that a sequence of evaluations. ordinal utility is kept in mind. present several heuristics .

%structure
  
  This paper is structured as follows. 
 



\section{Purchase Model Under Non-Compensatory Rules}\label{sec:model}

%Problem Definition
Suppose we have the item universe $V=\{v\}$ , the user universe $U=\{u\}$ , input data $D=\{d\}$ consists of a set of activity sessions. For now, we consider each activity session as a set of activities on items, $d=\{(v,n_v)\}$, where $n_v$ is the number of activities on item $v$.  In each activity session, at least one item will be purchased, thus $d$ can be divided to two disjoint sets of items, $d=W^d\bigcup L^d$, where $W^d$ is the set of purchases (winners), $L^d=d-W^d$ is the set of remaining items (losers). For example, if we are dealing with user activity sessions on an E-commerce website, $W^d$ will be the set of purchased items, $L^d$ will be the set of clicked but not purchased items, and $n$ is the number of clicks on $v$.


As with most factor models, we imagine that there are $K$ underlying aspects.  Without ambiguity, we use the same notion $u,v$ to denote user preferences and item features. $u,v\in \mathcal{R}^K$. We use subscripts to denote the elements in each vector, i.e. the preference value of user $u$ on the $k-$th aspect is $u_k$. 


The consumer $u$ starts an activity session $d$ with a motivation, which is most pertinent to aspect $k$. From a probabilistic perspective, we denote the most pertinent aspect by a $1-of-K$ coding scheme, $g\in \mathcal{R}^K, g_k=1,\forall k'\neq k, g_{k'}=0,$. The generation of $g$ is dependent on user preference, $g \sim Mult(u)$. Then the user will follow non-compensatory rules to select the purchased items. The probability of an activity session is denoted by the probability of generating all pairs of winners and losers in the session $p(d|\Theta, g)=\Pi_{w\in W^d, l \in L^d} p(<w,l>|\Theta,g)$, where $\Theta$ is other model parameters. Next we introduce the definition of $p(w|d,\Theta,g)$.

According to non-compensatory rules, we have the following two assumptions.

(1) The winner (purchased item) is superior on the most pertinent aspect $k$
(2)  The winner is not too bad on other aspects, compared with the losers (remaining items that are viewed but not purchased). 
  
Our model is inspired by the extended BTL model~\cite{Hunter2004MM} in social science. In the extended BTL model, a ranking $i \succ j$ is likely to happen if the latent utility score $i$ is relevantly larger than $j$, $p(i \succ j)=\frac{i}{i+ \theta j}$.  
  
 The parameter $\theta>1$ plays as a tolerance threshold. If the absolute difference between the $i$ and $j$ is not significant $|i-j|\leq \theta$, the user will consider it to be a tie $i=j$, $p(i = j)=\frac{(\theta^2-1)i j}{[i+\theta j][\theta i+ j]}$. Note that $p(i \succ j)+ p(i \prec j) + p(i=j) =1$. 
 
The usage of parameter $\theta$ can be interpreted as the consumer sets a minimally acceptable level of performance, which is controlled by both $\theta$ and how well another item $j$ performs, $i$'s performance must be at least better than this cutoff point. Therefore the probability of the winner $w$ being selected in the activity session $d$ is defined as the product of the probability that $w$ outranks other losing items $l$ on the most pertinent aspect $p(i_k\succ j_k)$ and the probability that $w$ at least tie with other items on other aspects $p(i_{k'} \preceq j_{k'})$: $p(<w,l>|g,\theta,V)  =  \Pi_{k=1}^{K}[ {\frac{w_k}{w_k+\theta v_k}}^{g_k} { \frac{\theta w_{k}}{v_{k}+\theta w_{k}}}^{1-g_k}]$. 

Thus we present the likelihood of purchase model under Non-Compensatory Rules (NCR) as follows.   

\begin{align}\label{equ:likelihood}
		p(D|\Theta)=\Pi_{d\in D} \Sigma_{g} \{\Pi_{w\in W^d, v\in L^d} p(<w,l>|g,\theta,V) p(g|u)\}
\end{align}

The model parameters are denoted as $\Theta=\{\theta,v\in V, u\in U)\}$.  The inference is implemented by computing a minorization function of the expectation $Q(\Theta)=E_g \ln p(D,G|\Theta) $ in an EM algorithm. For the limited space, we skip the derivation here and present the inference steps. 

In the E-step of $t-$th  EM round we compute $\gamma(d,k,\Theta^t)=p(g_k=1|d,\Theta^t)$ according to Equ.~\ref{equ:conditional}.

\begin{align}\label{equ:conditional}
\gamma(d,k,\Theta^t) &=\frac{u_k \Pi_{w \in W^d, v\in L^d} \frac{w_k}{w_k+\theta^t v_k}\Pi_{k'\neq k}\frac{\theta^t w_{k'}}{v_{k'}+\theta^t w_{k'}}}{\Sigma_{k=1}^K u_k \Pi_{w \in W^d, v\in L^d} \frac{w_k}{w_k+\theta^t v_k}\Pi_{k'\neq k}\frac{\theta^t w_{k'}}{v_{k'}+\theta^t w_{k'}}}
\end{align}

In the M-step, we update user preferences $u$, item features $v$ and minimal level of accepted performance $\theta$ by Equ.~\label{equ:update}. 

\begin{align}\label{equ:update}
u_k = & \frac{\Sigma_{u(d)=u}\gamma(d,k,\Theta^t)}{\Sigma_{s=1}^K \Sigma_{u(d)=u}\gamma(d,s,\Theta^t)} \\\nonumber
\frac{1}{v_k}= &\frac{\Sigma_{d\in W(v)}\Sigma_{v'\in L_d} [\frac{\gamma(d,k,\Theta^t)}{ \alpha(v,v',k,\Theta^t)} +\Sigma_{k'\neq k}\frac{\theta^t\gamma(d,k',\Theta^t)}{\alpha(v',v,k,\Theta^t)}]}{\Sigma_{d\in W(v)}|L_d|}\\\nonumber
 & + \frac{\Sigma_{d\in L(v)}\Sigma_{v'\in W_d} [\frac{\theta^t \gamma(d,k,\Theta^t)}{\alpha(v',v,k,\Theta^t)}+\Sigma_{k'\neq k} \frac{\gamma(d,k',\Theta^t)}{\alpha(v,v',k,\Theta^t)}] }{\Sigma_{d\in W(v)}|L_d|}\\\nonumber
\theta = & \frac{(K-1)\Sigma_d |W_d| |L_d|}{\Sigma_d \Sigma_k \gamma(d,k,\Theta^t)\Sigma_{w,v} [\frac{v_k}{\alpha(w,v,k,\Theta^t)}+\Sigma_{k'\neq k} \frac{w_{k'}}{\alpha(v,w,k',\Theta^t)}]}
\end{align}

One may argue that number of clicks $n_v$ matters. Intuitively, if a consumer constantly reviews his options, it means that the winner is more appealing and attracts more attentions. Therefore we present two variants of the NCR model: NCR-M and NCR-S.

In NCR-M, we construct $n_w \times n_l $ pairs of winners and losers, and compute the likelihood as in Equ.~\ref{equ:NCRM}. As $p(<w,l>) \leq 1$, this modification exaggerates the gap between $w$ and $l$ for $w$ with multiple occurrences. 

\begin{equation}\label{equ:NCRM}
		p(D|\Theta)=\Pi_{d\in D} \Sigma_{g} \{ \Pi_{w\in W^d, v\in L^d} {p(<w,l>|g,\theta,V)}^{n_w \times n_l} p(g|u) \}
\end{equation}

In NCR-S, the likelihood function is Equ.~\ref{likelihood}, but we modify the winner's probability by multiplying a sigmoid function $s(n_v)=\frac{1}{1+\exp n_v}$, as shown in Equ.~\ref{equ:preference}. Since $s(n_v)$ is monotonic, the more a winner appears, the larger the item feature $v$ should be to generate the observations. 

\begin{equation}\label{equ:peference}
 p(<w,l>|g,\theta,V)  =  \Pi_{k=1}^{K}[ {\frac{s(n_w) w_k}{s(n_w) w_k+\theta s(n_v) v_k}}^{g_k} { \frac{\theta s(n_w) w_{k}}{s(n_v) v_{k}+\theta s(n_w) w_{k}}}^{1-g_k}]
\end{equation}


\section{Prediction Strategy}\label{sec:strategy}

%problem definition: sequence of clicks
Given the input training data available as described in Section~\ref{sec:model}, our problem is that for a new activity sequence (i.e. click sequence) $\tilde{d}$,  where $\tilde{d}[i]$ is the item at the $i$-th position of sequence $\tilde{d}$,  output a ranking of items, so that the probabilities of purchasing these items are in descending order.
 
%basic strategy
Our basic strategy will be treating the activity sequence as a set of items, and directly apply the NCR model. The first step is to estimate the most possible aspect that the user in this session iis interested on, given the current browsing history $\tilde{d}$. We have

\begin{equation}\label{aprobability}
p(g_k=1|\tilde{d},V)=\frac{\Sigma_{v\in \tilde{d}} v_k}{\Sigma_k \Sigma_{v\in \tilde{d}} v_k}
\end{equation}

And the possibility of an object being selected is $\Pi_{v \in \tilde{d}} p(<w, v|g,\Theta>)$ 

%Strategy by sequence of evaluations

We present another type of strategy which is based on sequential evaluations. As discussed in Sec.~\ref{sec:intro}, our hypothesis is that the consumer conducts a series of alternative evaluations. The evaluation process starts when there are at least two alternatives, and ends at the last item of activity sequence. The evaluation process yields a evaluation sequence, only the ordinal message of evaluations $o$ is kept in memory. For example, as in Fig.`\ref{fig:illustration}, for a click sequence $(a,b,c,d,e,e)$, the consumer conducts 5 evaluations $(a,b), (a,b,c),(a,b,c,d),(a,b,c,d,e),(a,b,c,d,e,e) $. if the consumer adopts a non-compensatory rule as in model NCR with $\theta=0.05$, for comparison $(a,b)$, since $b$ is not superior on the pertinent aspect, the consumer is likely to choose $a$. For comparison $(a,b,c,d)$,  $d$ is below the cutoff point on the secondary aspect , so the consumer is tending to pick $c$. The ordinal message will be $o=(a,c,c,e,e)$. Based on how the consumer exploits the message $o$, we present three heuristic prediction strategies.

%ordinal utility

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Item & Pertinent Aspect & Secondary Aspect \\\hline
a & 0.6 & 0.4 \\
b & 0.4 & 0.6  \\
c & 0.64 & 0.36 \\
d & 0.78 & 0.22 \\
e & 0.65 & 0.35 \\\hline
\end{tabular}
\caption{A toy example of prediction strategy, with five evaluations with winners $a,c,c,e,e$}
\label{default}
\end{center}
\end{figure}\label{fig:illustration}

%strategy
(1) Addictive: In this strategy, the consumer cumulates all the ordinal information in each evaluation and selects the first item with the largest count. In Fig.`\ref{fig:illustration},  under the addictive strategy, the prediction is $c$. This strategy favors items that arrive first.

(2)Selective: In this strategy, the consumer selects the item with the largest frequency in $\tilde{d}$ if it is among the items with the largest cumulative counts. In Fig.`\ref{fig:illustration},  under the selective strategy, the prediction is $e$, as $e$ and $c$ both are ranked as number $\#1$ in the evaluations and $e$ appears for 2 times.

(3)Span: In this strategy, the consumer selects the item that achieves the maximal span. Definition of span. 



\section{Experiment}

\subsection{Experimental setup}

\subsection{Comparative Performance of Decision Rules} 

\subsection{Comparative Performance of Utility}


\section{Conclusion}

\section{Related Work}

\subsection{Rank Aggregation}
Due to its wide applications in ecomonics, biomedicines and social science, rank aggregation has been studied extensively in those communities. Researchers within these communities are often asked to find a consensus ranking, given a set of individual rankings over different alternatives. Consider a voting process in social science, each voter demonstrates a preferece list over political candidates, the goal is to achieve an optimal ranking to which each voter agrees in a sense.  In completing such a goal, there are usually two types of methods in literature. The first type is directly based on the ordinal data. Most commonly, the ordinal data is employed to construct a graph over alternatives, and a permutation of alternative positions can be achieved explicitly by finding the minimum feedback arc set~\cite{Alon2006Ranking}, in which case the resulting ranking has minimal Kendall Tau distance to all inputs; or by finding the stationary access distribution in a random walk framework~\cite{Negahban2012Iterative}, in which case the result approximates the MLE estimator of the BTL model. The second type is driven by random utility theory, as it assumes that a real-valued utility score is associated with each alternate, and the individual ranking is regarded as noisy observation of the ground truth ranking, which is the ordering of utility scores. The objective is thus transformed to inferencing the utility scores.         
%statistical models for generating rankings: pairwise, listwise and parameter estimation methods

In social choice and biomedical communities, popular random utility models include the Bradley-Terry-Luce model (BTL for short)~\cite{Hunter2004MM}, the Plackett-Luce model (PL)~\cite{AzariSoufiani2013Generalized}, Mallows models ~\cite{Lu2011Learning}, and so on. In the BTL model, suppose each individual is assigned a score $s_i$, the probability of pair-wise comparison $i>j$ is $p(i>j)=\frac{s_i}{s_j}$. In the PL model, the probability for a ranking $r=s_1 \succ s_2 \cdots \succ s_M$ is $ p(r|s)=\frac{s_1}{\Sigma_{l=1}^{M} s_l} \times \frac{s_2}{\Sigma_{l=2}{M} s_l} \cdots \times \frac{s_{M-1}}{s_{M-1}+s_{M}}$. The pairwise comparison probability is further studied in~\cite{Gleich2011Rank}. BTL can be regarded as a special case of PL via mrginalization. In the mallows model, the probability of ranking $p(r|\sigma,\phi)=\frac{1}{Z} e^{-\lambda d(r,\sigma)}$, where $\sigma$ is the hidden true ranking, $\lambda$ is the negative lognomial of a dispersion parameter, and $Z$ is the normalizer. Other extensions are also proposed under the random utility assumption, e.g. exponential distribution family is introduced in \cite{Parkes2012Random}.

We should notice that usually there is only one single ranking as output. It is believed that any rank aggregation requires some degree of compromise. The group nature of individual rankings is brought to attention in a recent study ~\cite{Wu2015Clustering}. In this study, a two stage strategy is adopted, where the inputs are clustered in a projected space, and then the true state is inferred in each cluster.
\bibliographystyle{abbrv}
\bibliography{C:/scratch/MyPaper/reference}

\end{document}

\documentclass[sigconf]{acmart}

\usepackage{booktabs} 
\usepackage{amsmath}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{algorithm2e}

\setcopyright{none}

\begin{document}

\title{A Psychological Model for Consumption Prediction}


\author{Paper 277}


\begin{abstract}
The study of consumer psychology reveals two categories of procedures used by consumers to make consumption related choices: compensatory rules and non-compensatory rules. Existing recommendation models such as matrix factorization assume the consumers follow the compensatory rules, which is to make decisions based on a weighted or summated score over different attributes. In this paper, we present a novel model which adopts non-compensatory decision rules. The selected item is superior on the most important attribute and beyond the minimally acceptable level on other attributes. Furthermore, we incorporate ordinal utility in the psychological model for consumption prediction in click sessions. We experimentally demonstrate that this model outperforms state-of-the-art methods.
\end{abstract}


\keywords{consumer psychology, non-compensatory decision rules, ordinal utility, early prediction}


\maketitle
\section{Introduction}\label{sec:introduction}
%consumer psychology: overview of decision rules


Ever since the dawn of psychological science, many attempts have been made to explain human choice behavior. It is well regarded that there are two categories of procedures to make consumption related choices: compensatory rules and non-compensatory rules~\cite{Engel1986Consumer}. Consumers who adopt compensatory rules evaluate every product over multiple relevant aspects and compute a weighted or summated score for each product. Then they will select the products with highest scores. The shortcomings of a product are balanced out by its attractive features. On the contrary, non-compensatory rules do not allow a good performance on one aspect of a product to compensate for poor performances on other aspects. Non-compensatory rules include lexicographic, conjunction and disjunction rules. Under lexicographic rules, products are compared on the most important aspect. Under conjunction and disjunction rules, the consumer imposes requirements for minimally acceptable values on each aspect separately. The conjunction and disjunction rules are often used in conjunction with other decision rules.
%previous works


One essential goal of recommender systems (RS) is to understand consumption patterns. In the fruitful literature of RS, almost all models are implementations of the compensatory decision rules, i.e. they assume that the probability of a consumption is calculated by a weighted combination of item performances over several hidden aspects~\cite{Hu2008Collaborative,Gopalan2015Scalable}. To the best of our knowledge, there is no model that is based on non-compensatory decision making rules.


However, many researches in the field of consumer psychology revealed that users use both compensatory and non-compensatory rules~\cite{Engel1986Consumer}. The selection of decision rules is affected by many factors. For example, a survey ~\cite{Park1976Effect} pointed out that when the consumer is familiar with the product and the product is low in complexity, he is most likely to establish a set of evaluative criteria and adopt conjunction rules.


%first contribution


Our first contribution in this work is a novel probabilistic model that represents the cognitive process of consumers following non-compensatory rules. The problem being tackled here is to predict what item the consumer will buy in an activity session (i.e. a click sequence on an e-commerce website). We assume that an item is purchased because (1) it is superior on the aspect which is currently of the most pertinence to the consumer, and (2) it surpasses a minimum level of acceptable performance on other aspects.


%challange to click sequence
We observe that in an activity session, sometimes the consumer clicks an item for multiple times at different time points. Multiple clicks could be the result of random error, repeated advertising exposures, or an intentional effort. Neither compensatory nor non-compensatory decision rules can explain how multiple clicks will affect choice.


%motivation of limited memory
According to study of consumer behavior~\cite{Engel1986Consumer}, consumers are with limited memory and attention. Our hypothesis in this paper is that a consumer's working memory is only capable of dealing with a limited number of alternatives. In an active session, the consumer will proceed with a series of alternative evaluations in his working memory. Once an evaluation is completed, the main message (i.e. which item is best within this bunch of alternatives) is retained in long-term memory, and the alternative portforlios (i.e. the details of items) are discarded.


%ordinal utility
Our second contribution in this work is to address the multi-click problem by introducing the concept of working memory. The decision rules are empolyed for alternatives within working memory. The main message of an evaluation is described by ordinal utility. In modern economics, ordinal utility theory is typically applied to study consumer behavior~\cite{Simon1959Theories}. Suppose there are two items to evaluate in working memory, the message to be persist is which item is better, but it is meaningless to keep in mind how much better it is. We present several heuristics to stimulate the strategy based on messages of several evaluations.


%structure

This paper is structured as follows. In section~\ref{sec:model} we describe the model based on non-compensatory decision rules. In section~\ref{sec:strategy} we discuss the evaluation process based on working memory. In section~\ref{sec:experiment} we analyze our experimental results. In section~\ref{sec:relatedwork} we briefly review the related work. In section~\ref{sec:conclusion} we present the conclusion and outlook of future work.




\section{Purchase Model Under Non-Compensatory Rules}\label{sec:model}

%Problem Definition
Suppose we have the item universe $V=\{v\}$ , the user universe $U=\{u\}$ , input data $D=\{d\}$ consists of a set of activity sessions. For now, we consider each activity session as a set of activities on items, $d=\{(v,n_v)\}$, where $n_v$ is the number of activities on item $v$.  In each activity session, at least one item will be purchased, thus $d$ can be divided to two disjoint sets of items, $d=W^d\bigcup L^d$, where $W^d$ is the set of purchases (winners), $L^d=d-W^d$ is the set of remaining items (losers). For example, if we are dealing with user activity sessions on an E-commerce website, $W^d$ will be the set of purchased items, $L^d$ will be the set of clicked but not purchased items, and $n$ is the number of clicks on $v$.


As with most factor models, we imagine that there are $K$ underlying aspects.  Without ambiguity, we use the same notion $u,v$ to denote user preferences and item features. $u,v\in \mathcal{R}^K$. We use subscripts to denote the elements in each vector, i.e. the preference value of user $u$ on the $k-$th aspect is $u_k$. 


The consumer $u$ starts an activity session $d$ with a motivation, which is most pertinent to aspect $k$. From a probabilistic perspective, we denote the most pertinent aspect by a $1-of-K$ coding scheme, $g\in \mathcal{R}^K, g_k=1,\forall k'\neq k, g_{k'}=0,$. The generation of $g$ is dependent on user preference, $g \sim Mult(u)$. Then the user will follow non-compensatory rules to select the purchased items. The probability of an activity session is denoted by the probability of generating all pairs of winners and losers in the session $p(d|\Theta, g)=\Pi_{w\in W^d, l \in L^d} p(<w,l>|\Theta,g)$, where $\Theta$ is other model parameters. Next we introduce the definition of $p(w|d,\Theta,g)$.

According to non-compensatory rules, we have the following two assumptions.

(1) The winner (purchased item) is superior on the most pertinent aspect $k$
(2)  The winner is not too bad on other aspects, compared with the losers (remaining items that are viewed but not purchased). 
  
Our model is inspired by the extended BTL model~\cite{Hunter2004MM} in social science. In the extended BTL model, a ranking $i \succ j$ is likely to happen if the latent utility score $i$ is relevantly larger than $j$, $p(i \succ j)=\frac{i}{i+ \theta j}$.  
  
 The parameter $\theta>1$ plays as a tolerance threshold. If the absolute difference between the $i$ and $j$ is not significant $|i-j|\leq \theta$, the user will consider it to be a tie $i=j$, $p(i = j)=\frac{(\theta^2-1)i j}{[i+\theta j][\theta i+ j]}$. Note that $p(i \succ j)+ p(i \prec j) + p(i=j) =1$. 
 
The usage of parameter $\theta$ can be interpreted as the consumer sets a minimally acceptable level of performance, which is controlled by both $\theta$ and how well another item $j$ performs, $i$'s performance must be at least better than this cutoff point. Therefore the probability of the winner $w$ being selected in the activity session $d$ is defined as the product of the probability that $w$ outranks other losing items $l$ on the most pertinent aspect $p(i_k\succ j_k)$ and the probability that $w$ at least tie with other items on other aspects $p(i_{k'} \preceq j_{k'})$: $p(<w,l>|g,\theta,V)  =  \Pi_{k=1}^{K}[ {\frac{w_k}{w_k+\theta v_k}}^{g_k} { \frac{\theta w_{k}}{v_{k}+\theta w_{k}}}^{1-g_k}]$. 

Thus we present the likelihood of purchase model under Non-Compensatory Rules (NCR) as follows.   

\begin{align}\label{equ:likelihood}
		p(D|\Theta)=\Pi_{d\in D} \Sigma_{g} \{\Pi_{w\in W^d, v\in L^d} p(<w,l>|g,\theta,V) p(g|u)\}
\end{align}

The model parameters are denoted as $\Theta=\{\theta,v\in V, u\in U)\}$.  The inference is implemented by computing a minorization function of the expectation $Q(\Theta)=E_g \ln p(D,G|\Theta) $ in an EM algorithm. For the limited space, we skip the derivation here and present the inference steps. 

In the E-step of $t-$th  EM round we compute $\gamma(d,k,\Theta^t)=p(g_k=1|d,\Theta^t)$ according to Equ.~\ref{equ:conditional}.

\begin{align}\label{equ:conditional}
\gamma(d,k,\Theta^t) &=\frac{u_k \Pi_{w \in W^d, v\in L^d} \frac{w_k}{w_k+\theta^t v_k}\Pi_{k'\neq k}\frac{\theta^t w_{k'}}{v_{k'}+\theta^t w_{k'}}}{\Sigma_{k=1}^K u_k \Pi_{w \in W^d, v\in L^d} \frac{w_k}{w_k+\theta^t v_k}\Pi_{k'\neq k}\frac{\theta^t w_{k'}}{v_{k'}+\theta^t w_{k'}}}
\end{align}

In the M-step, we update user preferences $u$, item features $v$ and minimal level of accepted performance $\theta$ by Equ.~\label{equ:update}. 

\begin{align}\label{equ:update}
u_k = & \frac{\Sigma_{u(d)=u}\gamma(d,k,\Theta^t)}{\Sigma_{s=1}^K \Sigma_{u(d)=u}\gamma(d,s,\Theta^t)} \\\nonumber
\frac{1}{v_k}= &\frac{\Sigma_{d\in W(v)}\Sigma_{v'\in L_d} [\frac{\gamma(d,k,\Theta^t)}{ \alpha(v,v',k,\Theta^t)} +\Sigma_{k'\neq k}\frac{\theta^t\gamma(d,k',\Theta^t)}{\alpha(v',v,k,\Theta^t)}]}{\Sigma_{d\in W(v)}|L_d|}\\\nonumber
 & + \frac{\Sigma_{d\in L(v)}\Sigma_{v'\in W_d} [\frac{\theta^t \gamma(d,k,\Theta^t)}{\alpha(v',v,k,\Theta^t)}+\Sigma_{k'\neq k} \frac{\gamma(d,k',\Theta^t)}{\alpha(v,v',k,\Theta^t)}] }{\Sigma_{d\in W(v)}|L_d|}\\\nonumber
\theta = & \frac{(K-1)\Sigma_d |W_d| |L_d|}{\Sigma_d \Sigma_k \gamma(d,k,\Theta^t)\Sigma_{w,v} [\frac{v_k}{\alpha(w,v,k,\Theta^t)}+\Sigma_{k'\neq k} \frac{w_{k'}}{\alpha(v,w,k',\Theta^t)}]}
\end{align}

One may argue that number of clicks $n_v$ matters. Intuitively, if a consumer constantly reviews his options, it means that the winner is more appealing and attracts more attentions. Therefore we present two variants of the NCR model: NCR-M and NCR-S.

In NCR-M, we construct $n_w \times n_l $ pairs of winners and losers, and compute the likelihood as in Equ.~\ref{equ:NCRM}. As $p(<w,l>) \leq 1$, this modification exaggerates the gap between $w$ and $l$ for $w$ with multiple occurrences. 

\begin{equation}\label{equ:NCRM}
		p(D|\Theta)=\Pi_{d\in D} \Sigma_{g} \{ \Pi_{w\in W^d, v\in L^d} {p(<w,l>|g,\theta,V)}^{n_w \times n_l} p(g|u) \}
\end{equation}

In NCR-S, the likelihood function is Equ.~\ref{likelihood}, but we modify the winner's probability by multiplying a sigmoid function $s(n_v)=\frac{1}{1+\exp n_v}$, as shown in Equ.~\ref{equ:preference}. Since $s(n_v)$ is monotonic, the more a winner appears, the larger the item feature $v$ should be to generate the observations. 

\begin{equation}\label{equ:peference}
 p(<w,l>|g,\theta,V)  =  \Pi_{k=1}^{K}[ {\frac{s(n_w) w_k}{s(n_w) w_k+\theta s(n_v) v_k}}^{g_k} { \frac{\theta s(n_w) w_{k}}{s(n_v) v_{k}+\theta s(n_w) w_{k}}}^{1-g_k}]
\end{equation}


\section{Prediction Strategy}\label{sec:strategy}

%problem definition: sequence of clicks
Given the input training data available as described in Section~\ref{sec:model}, our problem is that for a new activity sequence (i.e. click sequence) $\tilde{d}$,  where $\tilde{d}[i]$ is the item at the $i$-th position of sequence $\tilde{d}$,  output a ranking list of most possible purchases. This means that we should assign a numerical value for each item $score(v)$. 
 
%basic strategy
Our basic strategy will be treating the activity sequence as a set of items, and directly apply the NCR model to score each item.  The first step is to estimate the most possible aspect that the user in this session iis interested on, given the current browsing history $\tilde{d}$. We have

\begin{equation}\label{aprobability}
p(g_k=1|\tilde{d},V)=\frac{\Sigma_{v\in \tilde{d}} v_k}{\Sigma_k \Sigma_{v\in \tilde{d}} v_k}
\end{equation}

And the possibility of an object being selected is $score(v)=\Pi_{l \in \tilde{d}} p(<v, l>|g,\Theta>)$ 

%Strategy by sequence of evaluations

We present another type of strategy which is based on sequential evaluations. As discussed in Sec.~\ref{sec:intro}, our hypothesis is that the consumer conducts a series of alternative evaluations. The evaluation process starts when there are at least two alternatives, and ends at the last item of activity sequence. The evaluation process yields a evaluation sequence, only the ordinal message of evaluations $o$ is kept in memory. For example, as in Fig.`\ref{fig:illustration}, for a click sequence $(a,b,c,d,e,e)$, the consumer conducts 5 evaluations $(a,b), (a,b,c),(a,b,c,d),(a,b,c,d,e),(a,b,c,d,e,e) $. if the consumer adopts a non-compensatory rule as in model NCR with $\theta=0.05$, for comparison $(a,b)$, since $b$ is not superior on the pertinent aspect, the consumer is likely to choose $a$. For comparison $(a,b,c,d)$,  $d$ is below the cutoff point on the secondary aspect , so the consumer is tending to pick $c$. The ordinal message will be $o=(a,c,c,e,e)$. Based on how the consumer exploits the message $o$, we present three heuristic prediction strategies.

%ordinal utility

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Item & Pertinent Aspect & Secondary Aspect \\\hline
a & 0.6 & 0.4 \\
b & 0.4 & 0.6  \\
c & 0.64 & 0.36 \\
d & 0.78 & 0.22 \\
e & 0.65 & 0.35 \\\hline
\end{tabular}
\caption{A toy example of prediction strategy, with five evaluations with winners $a,c,c,e,e$}
\label{default}
\end{center}
\end{figure}\label{fig:illustration}

%strategy
(1) Addictive: In this strategy, the consumer collects the ordinal information in each evaluation and scores each item based on how often it is determined to be best. Thus $score(v)=\frac{|\{o[t] = v\}|}{|o|}$. Items with equivalent values are re-ordered in timestamp ascending order. In Fig.`\ref{fig:illustration},  under the addictive strategy, the output will be $c,e,a,b,d$. This strategy favors items that perform good and arrive first.

(2)Frequency: This strategy differs with addictive strategy in that, items with equivalent scores will be ranked by their frequencies in $\tilde{d}$. In Fig.`\ref{fig:illustration},  under the frequency strategy, the prediction is $e,c,a,b,d$.

(3)Span: This strategy presumes that consumers will be impressed by items that persistently win. The span of $s(v)$ is the longest continous subsequence of $o$ which contains only $v$, $\exists t,l, l\geq t, \forall t\geq j\leq l, o[j]=s(v)[j-t], \forall i, s(v)[i]=v$. The consumer will rank all items by the length of its span . In Fig.`\ref{fig:illustration}, under the span strategy, the prediction is $e,c,a,b,d$.


\section{Experiment}\label{sec:experiment}


\subsection{Experimental setup}


We evaluate our framework over a real data set, the YOOCHOOSE data set~\cite{Ben-Shimon2015RecSys}. The data set is a collection of user activity sessions for a large
European e-commerce website over six months. Each activity session consists of a sequence of click event, each click event includes time stamp, item ID, and item category. Each
record in yoochoose-buys.dat represents a purchase and includes
the following five fields: Session ID, Time Stamp, Item ID, Item
Price, and Quantity. There are 9,512,786 unique sessions from anonymous users.


The evaluation metrics are




\subsection{Decision Rules}


\begin{table*}
\caption{Comparative Performance of Decision Rules }
\label{tab:decision}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Performance}
& \multicolumn{2}{|c|}{Compensatory rules} & \multicolumn{3}{|c|}{Non-compensatory rules} & \multicolumn{3}{|c|}{Others} \\
\cline{3-10} & & BPR & MF & NCR & NCR-M & NCR-S & LinearRegression & GDBT & LogisticRegression \\\hline
\multirow{2}{*}{D1} &Precision	& & &	0.5717 & 0.6038 & 0.6354 &	0.6333 & 0.6423 &\bf{0.67}\\
\cline{2-10}	    &MRR		& & &0.7697 & 0.7887 & 0.8046 &0.8051 & 0.812	 &\bf{0.8244}\\\hline
\multirow{2}{*}{D2} &Precision  & & &0.6669 & \bf{0.7583} & 0.7371 &	0.6785 & 0.6878	& 0.6934\\
\cline{2-10}		 &MRR		& & &	0.8139 &\bf{0.8661} & 0.8545 & 0.8549 & 0.8597	& 0.8640\\\hline
\multirow{2}{*}{D3} &Precision & & & 0.6719 & \bf{0.7147} & 0.7061 &	0.6941 & 0.7079 & 0.7042\\
\cline{2-10} 		&MRR		& & & 0.8383 &\bf{0.8869} & 0.8721 & 0.8705 &0.8779 & 0.8788\\\hline
\multirow{2}{*}{D4} &Precision & & & 0.7014 & \bf{0.7360} & 0.7284 & 0.7188 & 0.7232 & 0.723\\
\cline{2-10} 		&MRR		& & & 0.8581& \bf{0.9044} & 0.8889 &	0.8852 & 0.8892& 0.8904\\\hline
\multirow{2}{*}{D5} &Precision	& &&	0.7192 &\bf{0.7522} &0.7461 & 0.7367 & 0.7423 &0.7402\\
\cline{2-10}		 &MRR		& && 0.8642 &\bf{0.9185} & 0.90246 & 0.9039 &	0.9065 & 0.9067\\\hline
\multirow{2}{*}{D6} &Precision	& && 0.7261 &\bf{0.7569} & 0.7506 & 0.7454 & 0.7512 & 0.7515\\
\cline{2-10}   		&MRR		& &&	0.8823& \bf{0.9221} &	0.9088 & 0.9053 & 0.9072 & 0.9072\\\hline
\end{tabular}
\end{table*}




\subsection{Evaluation Strategy}




\begin{table*}
\caption{Comparative Performance of Evaluation Rules }
\label{tab:decision}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Performance}
& \multicolumn{2}{|c|}{Cardinal Utility} & \multicolumn{6}{|c|}{Ordinal Utility} \\
\cline{3-10} & & Average & Sum & Ordinal-UA & Ordinal-US & Ordinal-UP & Ordinal-FA & Ordinal-FS & Ordinal-FP \\\hline
\multirow{2}{*}{D1} & Precision & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\cline{2-10} & MRR & 1 & 2 &3 & 4 & 5 &6 & 7 & 8
\\ \hline \end{tabular}
\end{table*}


\section{Conclusion}\label{sec:conclusion}


\section{Related Work}\label{sec:relatedwork}
The Recsys challange 2015 and Yochoos! data set.


\subsection{Rank Aggregation}
Due to its wide applications in ecomonics, biomedicines and social science, rank aggregation has been studied extensively in those communities. Researchers within these communities are often asked to find a consensus ranking, given a set of individual rankings over different alternatives. Consider a voting process in social science, each voter demonstrates a preferece list over political candidates, the goal is to achieve an optimal ranking to which each voter agrees in a sense.  In completing such a goal, there are usually two types of methods in literature. The first type is directly based on the ordinal data. Most commonly, the ordinal data is employed to construct a graph over alternatives, and a permutation of alternative positions can be achieved explicitly by finding the minimum feedback arc set~\cite{Alon2006Ranking}, in which case the resulting ranking has minimal Kendall Tau distance to all inputs; or by finding the stationary access distribution in a random walk framework~\cite{Negahban2012Iterative}, in which case the result approximates the MLE estimator of the BTL model. The second type is driven by random utility theory, as it assumes that a real-valued utility score is associated with each alternate, and the individual ranking is regarded as noisy observation of the ground truth ranking, which is the ordering of utility scores. The objective is thus transformed to inferencing the utility scores.         
%statistical models for generating rankings: pairwise, listwise and parameter estimation methods

In social choice and biomedical communities, popular random utility models include the Bradley-Terry-Luce model (BTL for short)~\cite{Hunter2004MM}, the Plackett-Luce model (PL)~\cite{AzariSoufiani2013Generalized}, Mallows models ~\cite{Lu2011Learning}, and so on. In the BTL model, suppose each individual is assigned a score $s_i$, the probability of pair-wise comparison $i>j$ is $p(i>j)=\frac{s_i}{s_j}$. In the PL model, the probability for a ranking $r=s_1 \succ s_2 \cdots \succ s_M$ is $ p(r|s)=\frac{s_1}{\Sigma_{l=1}^{M} s_l} \times \frac{s_2}{\Sigma_{l=2}{M} s_l} \cdots \times \frac{s_{M-1}}{s_{M-1}+s_{M}}$. The pairwise comparison probability is further studied in~\cite{Gleich2011Rank}. BTL can be regarded as a special case of PL via mrginalization. In the mallows model, the probability of ranking $p(r|\sigma,\phi)=\frac{1}{Z} e^{-\lambda d(r,\sigma)}$, where $\sigma$ is the hidden true ranking, $\lambda$ is the negative lognomial of a dispersion parameter, and $Z$ is the normalizer. Other extensions are also proposed under the random utility assumption, e.g. exponential distribution family is introduced in \cite{Parkes2012Random}.

We should notice that usually there is only one single ranking as output. It is believed that any rank aggregation requires some degree of compromise. The group nature of individual rankings is brought to attention in a recent study ~\cite{Wu2015Clustering}. In this study, a two stage strategy is adopted, where the inputs are clustered in a projected space, and then the true state is inferred in each cluster.
\bibliographystyle{abbrv}
\bibliography{C:/scratch/MyPaper/reference}

\end{document}
